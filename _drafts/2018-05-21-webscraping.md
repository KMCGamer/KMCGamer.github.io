---
layout: single
title: Webscraping a Wiki with Python
categories:
  - programming
tags: 
  - python
---

I started a new project called [Bladedrone](https://github.com/kmcgamer/bladedrone), which is an Angular 6 application for players of the free to play, first person shooter called ["Ironsight"](). My goal for this project is to allow users to view statistics on weapons, compare/sort them, as well as create & share custom builds. I am super excited about it and have been working on it almost every day!

## Getting Weapon Stats

In order to be able to sort and compare weapons, we kinda need the data, don't we? There are about 40-ish different weapons in the game with about 7-ish stats per gun. Let me tell ya, I was not about to record each individual stat into the database myself. Luckily, somebody had already done a lot of the work for me. I found a [wiki](http://ironsightgame.wikia.com/wiki/Ironsight_Wiki) for the game, and the author had written down the stats for each weapon! **Awesome!**

The wiki has a page for [all weapons](http://ironsightgame.wikia.com/wiki/Category:Weapon) with links to each individual one. The links then go to another page for each [specific weapon](http://ironsightgame.wikia.com/wiki/AK-12). On each weapon page is a short description of the weapon, a picture of it, its available attachments, and its statistics.

Problem was, there wasn't really a good way for me to retrieve this data. I mean sure, I could have copied and pasted the values into MongoDB manually with a bit of tweaking. And true, that method would have saved me a lot of time to begin with, but it still would have taken me a couple hours to complete. There had to be an even better way.

Let me introduce you to [Scrapy](https://scrapy.org/), a Python framework that allows you to extract data from websites! Scrapy is a web crawler where you can specify how it navigates, extracts, and parses the web and its data. It is super easy to learn, and a lot of fun to use! With this framework, I was able to tell the script to start at a certain URL, navigate to several links on the page, and extract the data that was on it. Additionally, I was able parse the data as it was extracting into JSON which saved me even more time with importing the data into MongoDB!

## Writing The Script

So lets get to building this thing shall we? The HTML for the "All Weapons" page is huge (probably because its generated by a CMS), but the part that we are interested in is this:

```html
<div lang="en" dir="ltr" class="mw-content-ltr">
  <table width="100%">
    <tr valign="top">
      <td width="33.3%">
        <h3>A</h3> <!-- Weapons starting with A -->
        <ul>
          <li><a href="/wiki/AK-12" title="AK-12">AK-12</a></li>
          <li><a href="/wiki/AK-47" title="AK-47">AK-47</a></li>
          <li><a href="/wiki/AN-94" title="AN-94">AN-94</a></li>
          <li><a href="/wiki/AR-57" title="AR-57">AR-57</a></li>
          <li><a href="/wiki/ARX160" title="ARX160">ARX160</a></li>
          <li><a href="/wiki/AUG_A3" title="AUG A3">AUG A3</a></li>
        </ul>
        <!-- So on and so forth -->
  </table>
</div>
```

So we know that all the links that we could possibly want are inside of a div element with a class of `mw-content-ltr`. Lets start coding the scraper, and tell it that we want to visit all of the links that are children of that div with that certain class. We are going to name this file `pullguns.py`.

```python
import scrapy

class WeaponSpider(scrapy.Spider):
    name = "weapon_spider"
    start_urls = ['http://ironsightgame.wikia.com/wiki/Category:Weapon']

    def parse(self, response):
        for link in response.xpath('//div[@class="mw-content-ltr"]//a/@href'):
            link = "http://ironsightgame.wikia.com" + link.extract()
            yield scrapy.Request(link, callback=self.parse_weapon)

    def parse_weapon(self, response):
      pass
```

We start off by creating a class with an arbitrary name to define how our spider will function. We then declare `start_urls` to contain all of our root urls. Since we only have one URL (the "all weapons" page) that we want to branch from, this will be a single element array. 

The `parse` method is created to implement how we want to traverse the webpage. We loop over the links contained inside that div with the class "mw-content-ltr". Now, I'm going to be completely honest, I don't really know how XPath works, but you can learn more about it [here](http://zvon.org/comp/r/tut-XPath_1.html). All I did was google for threads containing similar scenarios, and then modifying my query until it gave me what I wanted. Also, since the links for each weapon are not absolute, we append the link to the base url using `link.extract()`. Finally we tell scrapy to request the parsed link, and use the `parse_weapon` function as its callback for processing.

Now, if we run this in the command line using scrapy, we can take a look at what pages its crawling. To run scrapy, you use the command `scrapy runspider pullguns.py`. This is going to output a lot of debug information about how the web scraping is crawling our webpages. Here is the output for what we have so far (I modified it a little bit to save space):

<script src="https://gist.github.com/KMCGamer/dff1ea05654a97cbd78a02878a42f188.js"></script>

As you can see, the spider first visits parent URL that contains all of the weapon links we want to visit. After it does that, it visits each link individually, and returns a status code of 200.